# Pod theory
The atomic unit of scheduling in Kubernetes is the Pod.

## Why pods
There are three main reasons for Pods:
1. Pods augment containers
2. Pods assist in scheduling
3. Pods enable resource sharing  

### Pods augment containers
Pods augment containers in all the following ways:
1. Labels and annotations
2. Restart policies
3. Probes (startup probes, readiness probes, liveness probes, and potentially more)
4. Affinity and anti-affinity rules
5. Termination control
6. Security policies
7. Resource requests and limits  
  
Run a `kubectl explain pods` command to list all possible Pod attributes.  
  
#### Some of the pod features are:
- Labels let you group Pods and associate them with other objects in powerful ways.
- Annotations let you add experimental features and integrations with 3rd-party tools and services.
- Probes let you test the health and status of Pods and the apps they run.
> This enables advanced scheduling, updates, and more.
- Affinity and anti-affinity rules give you control over where in a cluster Pods are allowed to run.
- Termination control lets you gracefully terminate Pods and the applications they run.
- Security policies let you enforce security features.
- Resource requests and limits let you specify minimum and maximum values for things like CPU, memory, and disk IO.  

### Pods assist in scheduling
Labels, affinity and anti-affinity rules, as well as resource requests and limits give you fine-grained control over which worker nodes Pods can run on.  

### Pods enable resource sharing
On the sharing of resources front, Pods provide a shared execution environment for one or more containers.  
  
#### This shared execution environment includes things such as:
- Shared filesystem
- Shared network stack (IP address, routing table, ports…)
- Shared memory
- Shared volumes
> This means that if a Pod has two containers, both will share the Pod’s IP address and can access any of the Pod’s volumes to share data.  

### Static Pods vs controllers
#### There are two ways to deploy Pods:
1. Directly via a Pod manifest  
Pods deployed directly from a Pod manifest are called static Pods and have no superpowers such as self-healing, scaling, or rolling updates.  
> **This is because they’re only monitored and managed by the worker node’s kubelet process which is limited to attempting restarts on the local worker node.**  
<span style="color:red">**If the worker node they’re running on fails, there’s no control-plane process watching and capable of starting a new one on a different node.**</span>

2. Indirectly via a controller  
Pods deployed via controllers have all the benefits of being monitored and managed by a highly-available controller running on the control-plane.
> The local kubelet can still attempt local restarts, but if restart attempts fail, or the node itself fails, the observing controller can start a replacement Pod on a different worker node.  

### Deploying Pods
The process of deploying a Pod to Kubernetes is as follows:
1. Define it in a YAML manifest file
2. Post the YAML to the API server
3. The API server authenticates and authorizes the request
4. The configuration (YAML) is validated
5. The scheduler deploys the Pod to a healthy worker node with enough available resources
6. The local kubelet monitors it  
> If the Pod is deployed via a controller, the configuration will be added to the cluster store as part of overall desired state and a controller will monitor it.  

### The anatomy of a Pod
At the highest level, a Pod is an execution environment shared by one or more containers:
- net namespace: IP address, port range, routing table…
- pid namespace: isolated process tree
- mnt namespace: filesystems and volumes…
- UTS namespace: Hostname
- IPC namespace: Unix domain sockets and shared memory  

### Pods and shared networking
Every Pod has its own network namespace.  
This means every Pod has its own IP address, its own range of TCP and UDP ports, and its own routing table.
> In multi-container pods Container-to-container communication within the same Pod happens via the Pod’s localhost adapter and a port number. for example localhost:5000 or localhost:80  
